{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Hotel Review Aspect Summarization - Finetuning & Inference\n",
                "\n",
                "This notebook finetunes a language model for aspect-based hotel review summarization using Unsloth + LoRA.\n",
                "\n",
                "**Supported Models:**\n",
                "- `gemma3`: Gemma-3 270M Instruct (full precision)\n",
                "- `qwen25`: Qwen2.5 0.5B Instruct (4-bit)\n",
                "- `llama32`: Llama-3.2 1B Instruct (4-bit)\n",
                "\n",
                "**Data Recipes:**\n",
                "- `synth_100`: 100 synthetic samples (1800 training examples)\n",
                "- `human_25`: 25 human samples (450 training examples)\n",
                "- `mixed`: 75 synth + 25 human (1800 training examples)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Installation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, re\n",
                "import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
                "xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
                "!pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
                "!pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
                "!pip install --no-deps unsloth\n",
                "!pip install transformers==4.57.3\n",
                "!pip install --no-deps trl==0.22.2"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ============ CONFIGURATION ============\n",
                "# Change these settings as needed\n",
                "\n",
                "# HuggingFace Dataset\n",
                "HF_DATASET_REPO = \"thanh309/hotel-reviews-summarization\"  # <-- CHANGE THIS\n",
                "\n",
                "# Model: \"gemma3\", \"qwen25\", or \"llama32\"\n",
                "MODEL_KEY = \"gemma3\"\n",
                "\n",
                "# Data recipe: \"synth_100\", \"human_25\", or \"mixed\"\n",
                "RECIPE = \"mixed\"\n",
                "\n",
                "# Training settings\n",
                "MAX_SEQ_LENGTH = 20000\n",
                "NUM_EPOCHS = 1\n",
                "BATCH_SIZE = 1\n",
                "GRAD_ACCUM = 8\n",
                "LEARNING_RATE = 2e-4\n",
                "LORA_R = 16\n",
                "LORA_ALPHA = 32\n",
                "\n",
                "# Output settings\n",
                "OUTPUT_DIR = \"outputs\"\n",
                "OUTPUT_NAME = f\"{MODEL_KEY}_{RECIPE}\"\n",
                "\n",
                "print(f\"Configuration:\")\n",
                "print(f\"  HuggingFace Dataset: {HF_DATASET_REPO}\")\n",
                "print(f\"  Model: {MODEL_KEY}\")\n",
                "print(f\"  Recipe: {RECIPE}\")\n",
                "print(f\"  Output: {OUTPUT_NAME}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Download Dataset from HuggingFace"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from huggingface_hub import hf_hub_download, snapshot_download\n",
                "from pathlib import Path\n",
                "import json\n",
                "\n",
                "# Download all data files from HuggingFace\n",
                "print(f\"Downloading dataset from: {HF_DATASET_REPO}\")\n",
                "\n",
                "# Download training recipes\n",
                "RECIPE_FILES = {\n",
                "    \"synth_100\": \"recipes/synth_100.jsonl\",\n",
                "    \"human_25\": \"recipes/human_25.jsonl\",\n",
                "    \"mixed\": \"recipes/mixed.jsonl\",\n",
                "}\n",
                "\n",
                "# Download the selected recipe\n",
                "train_file = hf_hub_download(\n",
                "    repo_id=HF_DATASET_REPO,\n",
                "    filename=RECIPE_FILES[RECIPE],\n",
                "    repo_type=\"dataset\",\n",
                ")\n",
                "print(f\"Downloaded training data: {train_file}\")\n",
                "\n",
                "# Download test data\n",
                "test_file = hf_hub_download(\n",
                "    repo_id=HF_DATASET_REPO,\n",
                "    filename=\"space_summ_test.json\",\n",
                "    repo_type=\"dataset\",\n",
                ")\n",
                "print(f\"Downloaded test data: {test_file}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model & Data Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from datasets import load_dataset\n",
                "from unsloth import FastLanguageModel\n",
                "from unsloth.chat_templates import get_chat_template, train_on_responses_only\n",
                "from trl import SFTTrainer, SFTConfig\n",
                "from transformers import TrainerCallback\n",
                "\n",
                "# Model configurations\n",
                "MODEL_CONFIGS = {\n",
                "    \"gemma3\": {\n",
                "        \"model_name\": \"unsloth/gemma-3-270m-it-bnb-4bit\",\n",
                "        \"chat_template\": \"gemma-3\",\n",
                "        \"instruction_part\": \"<start_of_turn>user\\n\",\n",
                "        \"response_part\": \"<start_of_turn>model\\n\",\n",
                "        \"load_in_4bit\": True,\n",
                "    },\n",
                "    \"qwen25\": {\n",
                "        \"model_name\": \"unsloth/Qwen2.5-0.5B-Instruct-bnb-4bit\",\n",
                "        \"chat_template\": \"qwen-2.5\",\n",
                "        \"instruction_part\": \"<|im_start|>user\\n\",\n",
                "        \"response_part\": \"<|im_start|>assistant\\n\",\n",
                "        \"load_in_4bit\": True,\n",
                "    },\n",
                "    \"llama32\": {\n",
                "        \"model_name\": \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\",\n",
                "        \"chat_template\": \"llama-3.2\",\n",
                "        \"instruction_part\": \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
                "        \"response_part\": \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
                "        \"load_in_4bit\": True,\n",
                "    },\n",
                "}\n",
                "\n",
                "config = MODEL_CONFIGS[MODEL_KEY]\n",
                "use_4bit = config.get(\"load_in_4bit\", True)\n",
                "print(f\"Using model: {config['model_name']} (4-bit: {use_4bit})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training history callback\n",
                "class TrainingHistoryCallback(TrainerCallback):\n",
                "    def __init__(self, output_path):\n",
                "        self.output_path = Path(output_path)\n",
                "        self.history = {\"steps\": [], \"loss\": [], \"learning_rate\": [], \"epoch\": [], \"grad_norm\": []}\n",
                "    \n",
                "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
                "        if logs:\n",
                "            self.history[\"steps\"].append(state.global_step)\n",
                "            self.history[\"loss\"].append(logs.get(\"loss\"))\n",
                "            self.history[\"learning_rate\"].append(logs.get(\"learning_rate\"))\n",
                "            self.history[\"epoch\"].append(logs.get(\"epoch\"))\n",
                "            self.history[\"grad_norm\"].append(logs.get(\"grad_norm\"))\n",
                "            with open(self.output_path / \"training_history.json\", \"w\") as f:\n",
                "                json.dump(self.history, f, indent=2)\n",
                "    \n",
                "    def on_train_end(self, args, state, control, **kwargs):\n",
                "        print(f\"Training history saved to: {self.output_path / 'training_history.json'}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model\n",
                "print(f\"Loading model: {config['model_name']} (4-bit: {use_4bit})\")\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name=config[\"model_name\"],\n",
                "    max_seq_length=MAX_SEQ_LENGTH,\n",
                "    load_in_4bit=use_4bit,\n",
                ")\n",
                "\n",
                "# Add LoRA adapters\n",
                "print(\"Adding LoRA adapters...\")\n",
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r=LORA_R,\n",
                "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
                "    lora_alpha=LORA_ALPHA,\n",
                "    lora_dropout=0,\n",
                "    bias=\"none\",\n",
                "    use_gradient_checkpointing=\"unsloth\",\n",
                "    random_state=42,\n",
                ")\n",
                "\n",
                "# Apply chat template\n",
                "print(f\"Applying chat template: {config['chat_template']}\")\n",
                "tokenizer = get_chat_template(tokenizer, chat_template=config[\"chat_template\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load dataset from downloaded file\n",
                "print(f\"Loading dataset: {train_file}\")\n",
                "\n",
                "dataset = load_dataset(\"json\", data_files=train_file, split=\"train\")\n",
                "print(f\"Dataset size: {len(dataset)} examples\")\n",
                "\n",
                "# Format conversations\n",
                "def formatting_prompts_func(examples):\n",
                "    texts = [\n",
                "        tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
                "        for convo in examples[\"conversations\"]\n",
                "    ]\n",
                "    return {\"text\": texts}\n",
                "\n",
                "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
                "print(\"Dataset formatted!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create output directory\n",
                "output_path = Path(OUTPUT_DIR) / OUTPUT_NAME\n",
                "output_path.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Training config\n",
                "training_args = SFTConfig(\n",
                "    dataset_text_field=\"text\",\n",
                "    per_device_train_batch_size=BATCH_SIZE,\n",
                "    gradient_accumulation_steps=GRAD_ACCUM,\n",
                "    warmup_ratio=0.05,\n",
                "    num_train_epochs=NUM_EPOCHS,\n",
                "    learning_rate=LEARNING_RATE,\n",
                "    logging_steps=1,\n",
                "    optim=\"adamw_8bit\",\n",
                "    weight_decay=0.01,\n",
                "    lr_scheduler_type=\"cosine\",\n",
                "    seed=42,\n",
                "    output_dir=str(output_path),\n",
                "    save_strategy=\"epoch\",\n",
                "    report_to=\"none\",\n",
                "    fp16=not torch.cuda.is_bf16_supported(),\n",
                "    bf16=torch.cuda.is_bf16_supported(),\n",
                ")\n",
                "\n",
                "# Create trainer with history callback\n",
                "history_callback = TrainingHistoryCallback(output_path)\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    train_dataset=dataset,\n",
                "    args=training_args,\n",
                "    callbacks=[history_callback],\n",
                ")\n",
                "\n",
                "# Apply train_on_responses_only\n",
                "print(\"Applying train_on_responses_only...\")\n",
                "trainer = train_on_responses_only(\n",
                "    trainer,\n",
                "    instruction_part=config[\"instruction_part\"],\n",
                "    response_part=config[\"response_part\"],\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show GPU stats\n",
                "if torch.cuda.is_available():\n",
                "    gpu_stats = torch.cuda.get_device_properties(0)\n",
                "    print(f\"GPU: {gpu_stats.name}\")\n",
                "    print(f\"Total memory: {gpu_stats.total_memory / 1024**3:.1f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train!\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(f\"Starting training: {OUTPUT_NAME}\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "trainer_stats = trainer.train()\n",
                "\n",
                "print(\"\\nTraining completed!\")\n",
                "print(f\"  Runtime: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
                "print(f\"  Loss: {trainer_stats.metrics.get('train_loss', 'N/A')}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save model\n",
                "print(f\"\\nSaving model to: {output_path}\")\n",
                "model.save_pretrained(str(output_path))\n",
                "tokenizer.save_pretrained(str(output_path))\n",
                "\n",
                "# Save training config\n",
                "training_config = {\n",
                "    \"model_key\": MODEL_KEY,\n",
                "    \"model_name\": config[\"model_name\"],\n",
                "    \"chat_template\": config[\"chat_template\"],\n",
                "    \"recipe\": RECIPE,\n",
                "    \"max_seq_length\": MAX_SEQ_LENGTH,\n",
                "    \"lora_r\": LORA_R,\n",
                "    \"lora_alpha\": LORA_ALPHA,\n",
                "    \"num_train_epochs\": NUM_EPOCHS,\n",
                "    \"learning_rate\": LEARNING_RATE,\n",
                "    \"train_runtime_seconds\": trainer_stats.metrics['train_runtime'],\n",
                "    \"train_loss\": trainer_stats.metrics.get('train_loss'),\n",
                "}\n",
                "with open(output_path / \"training_config.json\", \"w\") as f:\n",
                "    json.dump(training_config, f, indent=2)\n",
                "\n",
                "print(\"Model saved!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Inference"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Constants for inference\n",
                "ASPECTS = [\"rooms\", \"location\", \"service\", \"cleanliness\", \"building\", \"food\"]\n",
                "\n",
                "SYSTEM_PROMPT = \"\"\"You are an expert abstractive summarizer. Your task is to write a summary of opinions for a specific aspect of a hotel.\n",
                "\n",
                "**CRITICAL RULES:**\n",
                "1. **NO META-LANGUAGE:** NEVER say \"Guests said,\" \"Reviewers mentioned,\" \"The consensus is,\" or \"Reports indicate.\"\n",
                "   - BAD: \"Guests found the location convenient.\"\n",
                "   - GOOD: \"The location is convenient.\"\n",
                "2. **DIRECT ASSERTIONS:** State opinions as objective facts.\n",
                "3. **BREVITY:** Keep it short (15-40 words), strictly under 75 words.\n",
                "4. **FOCUS:** Identify the main sentiment and primary reasons for it.\"\"\"\n",
                "\n",
                "def format_reviews_as_text(reviews):\n",
                "    paragraphs = []\n",
                "    for review in reviews:\n",
                "        sentences = review.get(\"sentences\", [])\n",
                "        if sentences:\n",
                "            paragraph = \" \".join(sentences)\n",
                "            paragraphs.append(f\"- {paragraph}\")\n",
                "    return \"\\n\".join(paragraphs)\n",
                "\n",
                "def create_messages(aspect, reviews_text):\n",
                "    instruction = f\"\"\"Summarize the **{aspect}** aspect from the following hotel reviews.\n",
                "\n",
                "Reviews:\n",
                "{reviews_text}\n",
                "\n",
                "Write a concise summary (15-40 words, strictly under 75 words) focusing only on {aspect}.\"\"\"\n",
                "    return [\n",
                "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
                "        {\"role\": \"user\", \"content\": instruction},\n",
                "    ]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Set model to eval mode\n",
                "model.eval()\n",
                "\n",
                "def generate_summary(aspect, reviews, max_new_tokens=256, temperature=0.7):\n",
                "    reviews_text = format_reviews_as_text(reviews)\n",
                "    messages = create_messages(aspect, reviews_text)\n",
                "    \n",
                "    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
                "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        output_ids = model.generate(\n",
                "            **inputs,\n",
                "            max_new_tokens=max_new_tokens,\n",
                "            temperature=temperature,\n",
                "            top_p=0.9,\n",
                "            do_sample=True,\n",
                "            pad_token_id=tokenizer.eos_token_id,\n",
                "        )\n",
                "    \n",
                "    input_length = inputs.input_ids.shape[1]\n",
                "    new_tokens = output_ids[0][input_length:]\n",
                "    return tokenizer.decode(new_tokens, skip_special_tokens=True).strip()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load test data\n",
                "print(f\"Loading test data: {test_file}\")\n",
                "with open(test_file, \"r\", encoding=\"utf-8\") as f:\n",
                "    test_entities = json.load(f)\n",
                "\n",
                "print(f\"Total test entities: {len(test_entities)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from tqdm.auto import tqdm\n",
                "\n",
                "# Run inference on all test entities\n",
                "print(f\"\\nRunning inference on {len(test_entities)} entities...\")\n",
                "results = []\n",
                "\n",
                "for entity in tqdm(test_entities, desc=\"Generating summaries\"):\n",
                "    entity_id = entity.get(\"entity_id\", \"\")\n",
                "    reviews = entity.get(\"reviews\", [])\n",
                "    golden_summaries = entity.get(\"summaries\", {})\n",
                "    \n",
                "    generated_summaries = {}\n",
                "    for aspect in ASPECTS:\n",
                "        summary = generate_summary(aspect, reviews)\n",
                "        generated_summaries[aspect] = summary\n",
                "    \n",
                "    results.append({\n",
                "        \"entity_id\": entity_id,\n",
                "        \"reviews\": reviews,\n",
                "        \"generated_summaries\": generated_summaries,\n",
                "        \"golden_summaries\": golden_summaries,\n",
                "    })\n",
                "\n",
                "print(\"Inference complete!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save results\n",
                "results_path = output_path / f\"{OUTPUT_NAME}_results.json\"\n",
                "print(f\"\\nSaving results to: {results_path}\")\n",
                "\n",
                "with open(results_path, \"w\", encoding=\"utf-8\") as f:\n",
                "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
                "\n",
                "print(\"Results saved!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Download Outputs\n",
                "\n",
                "Download the trained model and results:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# List output files\n",
                "print(f\"Output directory: {output_path}\")\n",
                "print(\"\\nFiles:\")\n",
                "for f in output_path.iterdir():\n",
                "    size = f.stat().st_size / 1024  # KB\n",
                "    print(f\"  {f.name}: {size:.1f} KB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create zip for download\n",
                "import shutil\n",
                "\n",
                "zip_path = f\"{OUTPUT_NAME}.zip\"\n",
                "shutil.make_archive(OUTPUT_NAME, 'zip', output_path)\n",
                "print(f\"\\nCreated: {zip_path}\")\n",
                "print(\"Download this file to get the trained model and results!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# For Colab: download the zip file\n",
                "# try:\n",
                "#     from google.colab import files\n",
                "#     files.download(zip_path)\n",
                "# except:\n",
                "#     print(\"Not in Colab. Zip file is available at:\", zip_path)\n",
                "print(\"Zip file is available at: \", zip_path)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Sample Output\n",
                "\n",
                "Preview some generated summaries:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Show sample output\n",
                "if results:\n",
                "    sample = results[0]\n",
                "    print(f\"Entity: {sample['entity_id']}\")\n",
                "    print(\"\\n\" + \"=\"*60)\n",
                "    print(\"GENERATED SUMMARIES:\")\n",
                "    print(\"=\"*60)\n",
                "    for aspect, summary in sample['generated_summaries'].items():\n",
                "        print(f\"\\n[{aspect.upper()}]\")\n",
                "        print(summary)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
